{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions on Use:\n",
    "1. To load **MA-BERT**, copy ```ma_bert``` folder into the ```transformers/models/bert``` folder.\n",
    "2. To load **MA-DistilBERT**, copy ```ma_distilbert``` folder into the ```transformers/models/distilbert``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neowe\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\neowe\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:82: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bert.ma_bert.modeling_ma_bert import MA_BertForMaskedLM\n",
    "from transformers.models.bert.ma_bert.configuration_ma_bert import MA_BertConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MA-BERT from Pretrained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifications Applied:\n",
      "Softmax Approximation: True, Share Softmax: False, Input Size: 128, Hidden Size: 128\n",
      "Normalization: Power, Warm-up Iterations: 997120, Accumulation Steps: 8\n",
      "Encoder Activation Function: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_bert_config = MA_BertConfig(verbose = True)\n",
    "ma_bert = MA_BertForMaskedLM(ma_bert_config)\n",
    "\n",
    "### Modify the checkpoint path if needed\n",
    "ma_bert_ckpt_path = \"ma_bert_mlm_ckpt.pt\" ### Assuming checkpoint is stored in the same directory\n",
    "ma_bert.load_state_dict(torch.load(ma_bert_ckpt_path, map_location = device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MA-BERT (Shared Softmax) from Pretrained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifications Applied:\n",
      "Softmax Approximation: True, Share Softmax: True, Input Size: 128, Hidden Size: 128\n",
      "Normalization: Power, Warm-up Iterations: 997120, Accumulation Steps: 8\n",
      "Encoder Activation Function: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### By default, a separate 2-layer neural network to approximate softmax is assigned to each encoder layer. \n",
    "### Set share_softmax_nn = True to allow the 2-layer neural network to be shared across all encoder layer\n",
    "ma_bert_config_shared_softmax = MA_BertConfig(share_softmax_nn = True, verbose = True)\n",
    "ma_bert_shared_softmax = MA_BertForMaskedLM(ma_bert_config_shared_softmax)\n",
    "\n",
    "### Modify the checkpoint path if needed\n",
    "ma_bert_shared_softmax_ckpt_path = \"ma_bert_mlm_ckpt_shared_softmax.pt\" ### Assuming checkpoint is stored in the same directory\n",
    "ma_bert_shared_softmax.load_state_dict(torch.load(ma_bert_shared_softmax_ckpt_path, map_location = device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MA-DistilBERT from Pretrained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.ma_distilbert.modeling_ma_distilbert import MA_DistilBertForMaskedLM\n",
    "from transformers.models.distilbert.ma_distilbert.configuration_ma_distilbert import MA_DistilBertConfig\n",
    "\n",
    "import torch\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modifications Applied:\n",
      "Softmax Approximation: True, Share Softmax: False, Input Size: 128, Hidden Size: 128\n",
      "Normalization: Power, Warm-up Iterations: 997120, Accumulation Steps: 8\n",
      "Encoder Activation Function: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_distilbert_config = MA_DistilBertConfig(verbose = True)\n",
    "ma_distilbert = MA_DistilBertForMaskedLM(ma_distilbert_config)\n",
    "\n",
    "### Modify the checkpoint path if needed\n",
    "ma_distilbert_ckpt_path = \"ma_distilbert_mlm_ckpt.pt\" ### Assuming checkpoint is stored in the same directory\n",
    "ma_distilbert.load_state_dict(torch.load(ma_distilbert_ckpt_path, map_location = device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
